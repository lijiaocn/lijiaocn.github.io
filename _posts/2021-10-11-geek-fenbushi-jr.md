---
layout: default
title: "《分布式金融架构课》阅读笔记：单机并发/多机并发/多副本读写正确性和一致性"
author: 李佶澳
date: "2021-10-11 11:35:08 +0800"
last_modified_at: "2021-10-11 11:35:08 +0800"
categories: 编程
cover:
tags: 阅读笔记 分布式
keywords: 事务,并发,一致性,可串行化,mvcc,多副本
description: 内容质量特别高！对单机并发、多机并发、多副本读写场景下的各种问题的阐述，让人脑塞顿开
---

## 目录

* auto-gen TOC:
{:toc}

## 说明

这个专栏的内容质量特别高！特别是对单机并发、多机并发、多副本读写场景下的各种问题的阐述，让人脑塞顿开。这篇笔记的章节分类是在对专栏内容反复阅读后整理出来的，和专栏中的讲解顺序不完全相同。

<span style="display:block;text-align:center">![仁杰《分布式金融架构》]({{ site.article }}/geek/jinrong.jpeg){: width="350px"}</span>

## 金融业务介绍

```
1. C端金融:
	1-1. 支付
2. B端金融——信贷类：
	2-1. 传统信贷业务
	2-2. 资产证券化
3. B端金融——交易类——场内交易：
	3-1. 交易所技术
	3-2. 算法交易平台
4. B端金融——交易类——场外交易：
	4-1. 合同定价与市场风险
```

## 示例：跨境电商扫码支付

跨行转账，白天只做记录，不进行实质性操作，每天结束的时候汇总两个银行的总转账金额，通过央行一次转账完成，该过程成为轧差，由清算机构负责。

1. 买家绑卡：向第三方支付提供账号、手机号，银行返回的验证码，第三方机构用这些信息从银行获取用户的唯一 token。
2. 买家转账：第三方支付用买家的 token 发起转账，从买家开户行转到第三方开户行 
3. 提交清算：第三方开户行向清算机构提交转账信息，清算机构记账完成后，通知买家开户行和第三方开户行
4. 扣款划账：买家开户行从买家账户扣除金额，第三方开户行为第三方账户增加金额
5. 轧差清算：晚上清算机构进行汇总，向央行发送银行间转账信息
6. 央行划账：央行划拨两家银行的准备金

第三方支付过程：

![第三方支付流程]({{ site.article }}/3tdpay.webp)

第三方跨境支付资金流动：

![外汇资金流动]({{ site.article }}/waihui.webp)

支付系统核心原理： **信息流和资金流分离，内部信息流系统和外部资金流系统异步交互**。

1. 支付环节的非银行参与者只产生信息流，不产生资金流
2. 信息流和资金流以不同的速度在不同时间不同的主体间分开流转，最终保持一致
3. 需要核算系统确认资金流和信息流的同步准确

第三方支付系统架构：

![第三方支付系统架构]({{ site.article }}/3thpay-arch.webp)


![资产证券化]({{ site.article }}/zichan.webp)

## 增加吞吐量的方法

**分库分表**是最常用的增加吞吐量的方法，使用分库分表时除了选择切割方法，还需要考虑引入的新问题： 

1. 多机器操作的正确性，需要实现分布式事务
2. 操作延迟，分布式事务至少两次网络沟通
3. 集群容灾，机器数目越多，出问题概率越高
4. 容量限制，分库分表一旦完成很难调整分库的数量 

**消息队列**是另一种常用增加吞吐的方法，适用于出处理峰值流量，消息队列的使用参考：[《消息队列高手课》阅读笔记：Rabbit/Rocket/Kafka/消息模型/消息事务/保序等](/编程/2021/10/09/geek-mq.html)。

## 降低延迟时间的方法

单机优化：

1. 使用单线程处理，去除资源抢占开销
2. 绑定/独占cpu，消除线程调度、CPU缓存更新的耗时
3. 实现内存池，消除内存申请分配的耗时
4. 操作文件系统尽量使用顺序写
5. 随机写场景中，用mmap将文件映射到内存页表，减少用户态和内核态的数据拷贝次数

网络优化：

1. 使用 epoll 应对高并发
2. 减少内核态用户态数据拷贝开销，譬如内核 io_uring 特性

![内核态用户态内存拷贝]({{ site.article }}/netcopy.webp)

## 对数据传输的要求

不同场景下对数据传输质量的要求不同，以券商和交易所的交互为例。

![券商和交易所系统]({{ site.article }}/quanshang-1.webp)

券商向交易所提交订单数据的场景：

1. 交易所处理券商发送的交易数据，要保证顺序正确，并且只处理一次（或幂等）
2. 交易所对券商发送的交易数据限流，漏桶算法 （直接丢弃）或令牌桶算法

交易所向券商下发行情数据场景，行情数据下发分为两种情况，实时下发和非实时下发：

1. 非实时下发使用「发布/订阅」模型，券商订阅交易所数据，可以使用 Kafka 等
2. 实时下发使用「同机房实时推送」方式，使用高效的传输协议 Google Prototol Buffer 或专用的 `FIX` 协议

>FIX协议只传输数据变动的部分

![交易实时推送行情数据]({{ site.article }}/quanshang2.webp)

实时推送与非实时订阅的结合：

![交易实时推送与非实时订阅]({{ site.article }}/quanshang3.webp)


## 数据库的选择

数据按照关系类型可以分为以下三种，分别对应三种类型的数据库：

1. 图数据类型： 图数据库
2. 没有关系的数据类型：时序数据库
3. 树状数据类型：关系型数据库

### 时序数据库

**金融市场数据一般都是时间序列数据，普遍使用时序数据库**。其中 [KDB][2] 是最主要的金融行业时序数据解决方案，适用数据范围 GB~TB，有专门的查询语言。

### 双时序数据库

**金融行业用一种特殊的`双时序数据库`，解决历史数据指标的查询问题。**

譬如 2018年4月（TT，记录时间） 公布了2018年3月（VT，发生时间）的 CPI 指数，粉红色区域是数据的可见范围。

![双时序数据库]({{ site.article }}/shuangshixu.webp)

双时序数据库中的数据可见范围，即「在特定时刻下查询某个数据指标」应该用哪份数据。

假设 2018年4月发布2018年3月的CPI，2018年7月发布2018年6月的CPI，2018年9月修正了2018年3月的CPI。在 2018 年10月查询2018年9月的CPI指标是，用目标时间所在方块的左下角数据：

>2018年10月时，9月的CPI数据还没有工作，使用最近一次的CPI数据，即 6 月份的数据

![双时序数据库的数据可见范围]({{ site.article }}/shuangshixu2.webp)


### 关系型数据库

关系型数据库具有「对象关系阻抗不匹配（Object relational impedance mismatch）」，将对象数据存放到关系型数据时需要需要进行额外的翻译，两种数据描述方式互相冲突。

1. 面向对象编程的对象之间关系形成图，用数学上的图论研究
2. 关系数据库基于关系代数，是同构列表组成的集合，用数学上的集合论研究
3. 将对象存储到关系数据库的过程，是将图论翻译到集合论的过程

NewSQL/NoSQL 试图解决对象关系阻抗不匹配，解决了对象的存储问题，没有完美解决对象的查询问题。NewSQL普遍采用分布式架构，二级索引一般采用最终一致性，会导致查询不准。

**金融行业主力使用的还是关系型数据库，单机事务极大简化了架构难度**。迫不得已的时候，才会使用分布式存储，升级成 异步处理架构。

## 金融系统运行正确性保证

事件溯源（Event Sourcing）是金融行业实践中采用的正确性解决方案，分别在事前、事中、事后采取措施保证正确性。

### 事件溯源设计

事件溯源中，将所有操作（无论多么复杂）都拆解成`命令`、`事件`和`状态`的组合。

![命令/事件/状态]({{ site.article }}/event-source-1.webp)

把一系列复杂的操作，用下面的简单组合描述：

![复杂操作拆解]({{ site.article }}/event-source-2.webp)


转账操作的事件描述方式：

![转账的事件组合]({{ site.article }}/event-source-3.webp)

事件溯源要求所有所有命令/事件都要有确定顺序，如果顺序变化了，就是另一种结果。用队列存储保障顺序：

![命令队列/事件队列]({{ site.article }}/event-source-4.webp)


事件溯源的顺序要求，确定了队列存储可以顺序写磁盘文件的的方式实现：

![命令队列/事件队列的实现]({{ site.article }}/event-source-5.webp)

解决了队列问题后，再需要解决的是事件执行问题，事件执行使用「自动机模型」：

![事件执行自动机]({{ site.article }}/event-source-6.webp)

**自动机模型对「正确性」非常关键**，自动机模型必须做到：只要开始状态一致，相同事件队列的执行结果一定是相同的。只有做到这一点，后面的「时光机」和「事件容载」才能成立。要实现这一点需要需要避免以下操作：

1. 不要使用每次执行都会产生不同结果的随机数
2. 不要有来自的外部的不确定输入

完成以上设计后，可以用「时光机」进行审计，使用事件进行冗灾.

**时光机审计**：可以回退到任意一个过去时刻，重新检查每一步的执行情况。

**事件容灾**：如果状态数据全部丢失了，只要事件还在，就可以从初始状态恢复出数据。

为了缩短事件容灾恢复的事件，可以使用「快照」技术，从快照开始恢复。
金融系统中通常每晚12点进行「日切」，并且按月、季度、年份进行清点。

![从快照开始恢复]({{ site.article }}/event-source-7.webp)

事件溯源需要专门设计事件查询方法， 支持查询任意时间的状态，通过把事件加载到一个读模式自动机实现：

![事件状态查询]({{ site.article }}/event-source-8.webp)

### 事件溯源的本质：不可变架构

事件溯源实际是一种不可变架构/Immutable Architecture，在不可变架构中，数据只能追加写入，不能修改，数据一旦 生成，都不能发生变化。数据分为「事件」和「状态」两类，任何一个事件点的状态等于之前所有事件效果的累计。

![不可变架构]({{ site.article }}/event-source-9.webp)

## 单机操作的正确性保证

对单机上的数据进行操作时，面临两个问题：

1. **操作到一半时发生故障，数据只改写了一半，另一半未改写，要怎么办？**

2. **「实际由多个步骤组成」的原子操作并发执行时，结果应该是怎样的？**

第一个问题解决方法是提供「原子操作」。

面向磁盘读写时，假设要在磁盘上写 4k 数据，写完 2k 后机器断电，之后从磁盘读取的是改写了一半的数据，磁盘无法提供原子保证。如果开发一个直接读写磁盘的服务，必须考虑数据操作的原子性。

数据库是一个典型的例子，它构建在磁盘之上，对用户屏蔽了磁盘操作。磁盘上依然会存在更新了一半的数据，但是数据库通过一系列设计让用户读不到这些不完备的数据。用户面向数据库读写时，数据操作是原子的。依此类推，除了数据库，还有那些系统需要或提供了类似的原子保证？如果要自行实现原子操作，需要怎么做？

定义了原子操作后，引出了第二个问题，怎样规定并发行为。

原子操作是一个逻辑概念，它指的是「一系列动作的最终结果是原子的」。在一个原子操作中，会有很多个数据处理步骤。当很多个原子操作并发执行时，这些原子内部的操作可能会同时操作同一段数据，这时候要遵循怎样的规则？

数据库给出的解决方法是定义了四种隔离级别，即四种规则，有用户根据实际情况选择。

## 单机操作的并发问题

单机上的数据操作要保证正确，主要的难点在于如何解决并发操作时的冲突，总共有三种冲突场景：

1. 写写
2. 写读
3. 读写

**写写冲突**，事务 T2 还在执行时，就被另一个事务覆写了——脏写/Dirty Write，：

![写写冲突]({{ site.article }}/conflict-1.webp)

**写读冲突**，事务 T2 执行是读取到了 T1 改写的数值，但是随后 T1 回滚，T2 相当于读了一个不存在的数值——脏读/Dirty Read：

![写读冲突]({{ site.article }}/conflict-2.webp)

**读写冲突**，事务 T1 第一次读取到数值，被另一个事务改写，事务T1 再次读取时，读到一个另一个数值——模糊/Fuzzy Read：


![读写冲突]({{ site.article }}/conflict-3.webp)

为了消除以上三种冲突，分别提出了几种方案：

1. 读未提交，Read Uncommited，能够读到事务已改写但未提交的的数据，相当于无隔离，什么冲突也没解决
2. 读已提交，Read Commited，事务尚未提交的改动不允许读取，解决了 Dirty Read 
3. 可重复读，Repeatable Read，事务执行期间的多次读取到的数据是相同的，解决了 Fuzzy Read
4. 可串行化，Serializability，多个事务并发执行的结果等同于将事务串行执行，不会有任何冲突问题
5. 严格可串行化，Strict Serializability，多个事务并发执行的结果等同于按照事务的发生时间排序的串行执行，不会有任何冲突


![事务隔离级别]({{ site.article }}/conflict-4.webp)


### 可串行化定义和实现方法

事务隔离的最高级别「可串行化」不是要求所有事务必须串行执行，而是要求`事务并发执行的结果等价于这些事务按顺序串行执行的结果`，只要求结果等价，不要求实际串行。如何实现等价，是系统设计的问题。

例如，下面是两个并发执行的事务，两个事务的持续时间是重叠的：

![并发执行的事务]({{ site.article }}/chuanxing-1.webp)

如果他们的执行结果等价于下面的操作，就是可串行化：

![事务可串行化]({{ site.article }}/chuanxing-2.webp)

#### 两阶段加锁：冲突可串行化

可串行化的一种实现方式是加锁，这种方式叫做 2PL（Two Phase Lock，两阶段锁），即在事务开始前对所有要访问的资源添加排他锁，操作完成后再解锁。 **加锁的方式实现的可串行化，实际上是一种更严格的「冲突可串行化」，2PL方案不仅解决串行化问题，还解决了冲突问题。**

`可串行化不等于无并发冲突`。譬如下面的三个事务存在冲突，但是可以串行化：

![可串行化的事务]({{ site.article }}/conflict-5.webp)

串行化调整结果，t1、t2、t3 串行执行结果和并发执行结果是相同的：

![可串行化调整]({{ site.article }}/conflict-6.webp)

虽然存在冲突，但是这三个事务并发执行的最终结果，等价于串行执行。虽然 t1 和 t2 对 y 的写操作冲突，t1、t2和t3 对 x 写操作也冲突，但是结果依然等价于串行执行。

**冲突可串行化**要求`两个事务之间不能有冲突`，比可串行化更严格，是更彻底的串行化。按照冲突可串行化要求，上面的例子中 t1、t2 存在写冲突，能串行化但是不是冲突可串行：

![存在冲突不是冲突可串行]({{ site.article }}/conflict-7.webp)

思考以下，如果用加锁的方式实现可串行化，实际得到的更严格的冲突可串行化。但是`可串行化就足够了，没必要做到冲突可串行`，冲突可串行会导致并发性能下降。

#### 快照隔离MVCC：可串行化

MVCC机制保留了数据的多个版本，根据事务的开始时间，确定每个事务应该看到的数据版本。它是 **通过修改数据操作的义，消除了大量了读写冲突**，即要被操作的不是每个数据的最新版本，而是指定版本，如果两个并发动作分别操作同一数据的不同版本，定义为没有冲突，只有操作同一数据同一版本时（当前读场景），才用加锁的方式实现串行。

mvcc给每个数据引入了纵深，如下图所示，没有 mvcc 之前，dataA 只有一个value，op1/op2/op3 操作的是dataA 彼此构成冲突。引入 mvcc 之后，dataA 可有有三个 value，当 op1/op2/op3 分别操作的是 dataA 的三个版本时，彼此没有冲突。

```
      data A （before mvcc）                data A （after  mvcc）

           +--------+                        +----+---------+
           | valueA |                        | v3 | valueA3 | <----- op3
         > +--------+ <                      +----+---------+
       -/      ^       \-                    | v2 | valueA2 | <----- op2
     -/        |         \-                  +----+---------+
    /          |           \                 | v1 | valueA1 | <----- op1
   op1        op2         op3                +----+---------+

           conflict                               no conflict
```

MVCC 是引入了一种新的数据操作定义，不止在可串行化用到，在其它隔离级别也有使用：

![快照隔离]({{ site.article }}/conflict-8.webp)

#### 严格可串行化

另外还有一种 Strict Serializability，严格可串行化对事务的串行化的顺序也提出了要求， **必须和按照事务的提交顺序的串行执行结果一致**（可串行化只要求和某个串行执行结果一致）。严格可串行化运行效率特别低，一般很少用到。

## 多机操作的正确性保证

多机数据操作的正确性构建在单机数据操作正确性之上，多机场景下产生了新的问题： **部分机器上操作成功，部分机器上操作失败， 由多台机器构成的数据整体只改写了一半，要怎么办？**

参照单机的处理思路，可以想到的一种处理方法是：实现分布式原子操作，即分布式事务。分布式事务实现有多种方式，譬如下面的 2PC 和 TCC。无论用哪种方式实现，都会再次引入两个问题： **怎样处理乱序**， **怎样处理并发冲突**。

### 方案1：两阶段提交（2PC）

2PC，Two-Phase Commit，是一个解决多机数据操作正确性问题的方法（分布式协议），不同的系统可能有不同的实现方式，譬如分布式数据库和具体场景下的分布式业务，两阶段的具体操作可能是不同的。

2PC的核心思想是引入一个「协调者」，将多机数据操作分为「更改」和「提交」两步。第一步，协调者向目标机器发出数据操作请求，如果有一台机器失败或超时，事务失败。第二步，协调者收到所有机器的修改完成响应后，再次向所有机器发送提交指令，如果有一台机器提交失败或超时，事务失败。

借用 [Two-Phase Commit (2PC)][3] 的说明图：

第一阶段：发送更改指令，统计响应结果：

![两阶段提交第一阶段]({{ site.article }}/2pc-1.png)

第二阶段：发送提交指令，统计响应结果：

![两阶段提交第二阶段]({{ site.article }}/2pc-2.png)

多机操作中使用两阶段提交协议实现事务时，事务中间状态的问题更加突出。需要多机操作并通过网络进行交互确认，事务中间态的持续时间被拉长，中间态时要怎样控制每台机器上的数据的可见性，以及协调者宕机时，处于中间态的事务要怎样处理，需要有明确的规则。

### 示例：使用2PC实现分布式转账

任杰在《分布式金融架构课》中给出了一个用 2pc 协议的分布式转账案例。

转出账户 x 位于数据库 A 中，转入账户 y 位于数据库 B 中，x 和 y 的账户数据分布在两台机器上。现在要从 x 转出 100 元到 y。

![分布式转账]({{ site.article }}/2pc-example-1.webp)

如果 x y 位于同一个数据库中，用下面的事务语句即可，假设只需要更新账户余额，x 账户初始余额 100 元，y 账户初始余额 0 元，转账就是 x 账户清零，y 账户增加 100：

```sql
begin transaction
  update A set balance=  0 where accountID='x'
  update B set balance=100 where accountID='y'
end transaction
```

现在，x 和 y 位于两台机器上的两个数据库中，不能用单机事务语句，两阶段的实现方法如下：

**1. 协调者发送变更指令 ：**

![两阶段提交步骤1]({{ site.article }}/2pc-1.webp)

**2. 协调者等待目标机器确认：**

![两阶段提交步骤2]({{ site.article }}/2pc-2.webp)

**3. 协调者发送提交指令：**

![两阶段提交步骤3]({{ site.article }}/2pc-3.webp)

**4. 协调者等待目标机器确认：**

![两阶段提交步骤4]({{ site.article }}/2pc-4.webp)

站在协调者角度，整个过程需要和每台机器进行两次网络往返通信。2PC 最大问题是性能，2pc 协议中，当时事务处于中间状态时，每个机器上对应的数据操作是阻塞的。

### 方案2：Try-Confirm-Cancel

TCC，Try-Confirm-Cancel，是另一个解决多机数据操作正确性问题的方法（分布式协议）。TCC 也是有一个协调者，然后分两阶段处理的。协调者先发送操作指令，等所有机器确认后，再发送提交指令。

和 2PC 不同的是，TCC 中目标机器只负责执行本地事务，感知不到分布式的全局事务，TCC 发送给目标机器的操作执行是执行完成一个本地事务，分布式事务执行失败时，TCC 向目标机发送的指令是再执行一个反向的本地事务。

2PC 实现的是一个更严格的分布式事务，目标机器需要参考全局事务的执行状态，确定本地数据是否可见，TCC 实现的分布式事务不那么严格，在全局事务的中间状态，目标机器上的本地数据是可见的。

2PC 适用于要对外提供事务功能的分布式系统，譬如分布式数据库，用户只需要开启事务、执行语句、提交事务，不需要关心事务的回滚过程。

TCC 适用于业务系统开发，用户要定义两组操作：1. 正向的数据变更动作；2. 负向的数据变更动作。

### 示例：用 TCC 实现的分布式转账

《分布式金融架构课》中的例子，使用 TCC 协议实现分布式转账时，定义的两个阶段的两组操作如下。

目标机器 A 和 B 上的正向操作：

1. 第一阶段：A 机器完成 x 账户 -100 的本地事务，B 机器执行空事务（什么也不做），x 和 y 账户余额都为 0

2. 第二阶段：A 机器执行空事务（什么也不做），B 机器完成 y 账户 +100 的本地事务，x 和 y 账户余额回归正确值

![TCC转账两阶段操作内容]({{ site.article }}/tcc-1.webp)

目标机器 A 和 B 上的反向操作：

1. 第一阶段的撤销：第一阶段失败，只能是 A 机器的本地事务执行失败。A 机器上的数据未变化，B机器上无操作，撤销操作为取消 B 机器的空事务。A 和 B 机器上的数据都没有变化，撤销时不需要执行数据修改操作。

2. 第二阶段的撤销：第二阶段失败，只能是 B 机器的本地事务执行失败。A 机器上的数据在第一阶段被修改，B 机器数据一直未发生变化，撤销时需要给 A 机器发送一个执行反向操作的指令。只在 A 机器上修复数据。

![TCC转账两阶段操作的取消过程]({{ site.article }}/tcc-2.webp)


在转账这个案例中，每个阶段只对一个机器进行实质操作，并且先操作被扣款的帐户所在的机器。这种设计巧妙之处是，当全局事务处于中间状态时，x 的余额为 0，y 的余额也为 0，这种中间状态数据可见时，处理系统中的总金额减少了 100，不会导致其它不可接受的错误。

举个反例，如果先对 y 账户 +100，在对 x 账户 -100，全局事务的中间状态为 x 账户余额 100，y 账户余额 100。这个状态对外可见，意味着在另外一个并发执行的事务中， x 账户还有余额可以花费，y 账户也有余额可以花费，另外一个事务不仅可以对 x 继续划账，还可以对 y 继续划账。如果 x 到 y 的最终转账没成功，或者另一个事务先把 x 上的余额扣的不足了，需要连串的撤销已经提交成功的事务。如果先扣减 x 账户，中间状态时，第二个并发事务不可能成功或不会产生不可容忍的错误。

## 多机操作的乱序和并发问题

无论是 2PC 段和还是 TCC，对每台机器都需要进行两步操作，并且操作指令是通过网络传输。只要通过网络传输就会存在乱序问题。

譬如在一个分布式事务中， B 机器没有收到第一阶段的变更指令，协调者认定超时后，发出了取消事务的指令，B 机器先收到取消指令，然后才收到了最初的变更指令。

![TCC的乱序问题]({{ site.article }}/tcc-3.webp)

除此之外，多机数据操作同样存在并发问题。

### 示例：TCC 实现分布式转账的乱序和并发

前面实现的 TCC 分布式转账对乱序问题的解决方法：**让每台机器意识到乱序发生了，并采取行动**。

譬如对 B 机器而言，当它先收到一个取消指令时，能够通过检索本地记录发现这个取消指令没有对应的变更指令，是一个后发先到的乱序指令。B 机器这时候进行「空回滚」，并且记录下这个空回滚指令。等先发后到的变更指令到达时，B 机器再次检索本地记录，发现这个指令有对应到空回滚操作，拒绝处理变更（这个动作叫做防悬挂）。

TCC 对并发的解决方法是：**仔细规划两阶段里操作，保证在事务的中间态时，另一个事务的能够执行**。

能够执行是指，在事务 A 处于中间态的时候执行事务 B，不会出现不能容忍的错误。

## 多机数据备份的一致性保证

「一致性」这个词至少有两个使用场合：

1. 数据库ACID模型中的一致性：读取的数据要么是新数据，要么是旧数据，不能是两者的混杂，[ACID][3]。

2. 数据多副本冗灾中的一致性：主节点和副本的各种读写组合的结果的约定。

这里要研究的是数据多副本时的一致性问题。

首先要分辨下概念，「多机数据操作」和「多机数据备份」是完全不同的问题。

1. 多机数据操作：一份数据被拆分成多块分布在多台机器上时，要如何操作才能保证数据的正确性。

2. 多级数据备份：同一份数据延迟复制到了多台机器上，随机读取主副本机器，读取的是新数据还是旧数据。

多机数据备份还需要考虑同机房/跨机房、同城/异地等实施场景。

### 多机数据备份的 CAP 理论框架

CAP 是一个经过数学证明的结论：集群发生网络故障，脑裂成为无连接的两个集群后，要么所有集群不可用（Availability不满足），要么集群可用但是数据不保证一致（Consistency）。

CAP 给出的指导意见是：`当脑裂发生后，需要做的是选择怎样的牺牲方式，是牺牲可用性，还是牺牲一致性`。

需要注意的是，任杰在专栏中指出： **CAP 对一致性的定义太过简单，学术界已经不太建议用这个术语。**

分布式环境下多副本数据的一致性有几十种分类，CAP 使用的一致性定义是「可线性化（Linearizability）」，区分度不够，并不是所有场景下都需要「可线性化」的一致性。

下面介绍几种不同场景下的一致性：

```sh
1. 最终一致性
2. 单会话场景：
	2-1: 单调写一致
	2-3: 单调读一致
	2-3：单调读一致
3. 多会话场景：
	3-1：先读后写
	3-1：线性一致
```

#### 理论：最终一致性

最终一致性，对主节点的数据修改操作，最终会在副本机器上可见。最终一致性没有给出「最终」的时间范围，在工程上上的指导意义不大。

#### 单会话：单调写一致

多机备份中，同一个用户的请求可能会被分发到不同的副本机器上，数据变更扩散到副本机器上是需要时间的，需要定义在一次「会话/Seesion」中的一致性标准，关键点是用户从副本读取到的数据是更新前的还是更新后的。

Monotonic Write，所有备份机器上的多次数据写入操作的执行顺序，和用户在主节点上的操作顺序是相同的。

下图是不符合单调写一致性的场景，副本机器上的写入顺序和主节点不同：

![非单调写一致性]({{ site.article }}/yizhixing-1.webp)


#### 单会话：单调读一致

Monotonic Read，同一个会话中的多次读取，排在后面的读操作读取的数据，不能比前面的读操作获取的数据的版本老。即一旦读出一份数据后，后面的重复读取，只能读到比当前新的版本，不能读到比当前老的版本。

下图是违背单调读一致的场景，副本的更新有延迟，用户第一次从主节点读取到更新后的数据，第二次从副本读取时，副本你还没有收到完成更新，用户读到了老版本的数据：

![非单调读一致性]({{ site.article }}/yizhixing-2.webp)


#### 单会话：自读自写

Read Your Write，同一个会话中进行连续写读操作时，读取的数据是多个版本，分别对应这之前的每次写入操作的结果。即读取出来的不是一份数据，而是一份数据的多个版本的数值。这样设计的目的是让用户能自行判断出数据的新旧。

下图是违背自读自写的场景，用户从副本读取时，副本还没收到第二次更新，用户读不到之前的所有版本：

![非自读自写]({{ site.article }}/yizhixing-3.webp)

#### 多会话：先读后写

多会话一致性，研究的是对同一份数据进行并发读写时的结果约束。

Write follow Reads，一个会话读取到了前一个会话的修改的结果，那么当前会话的写操作一定要在前一个会话的后面的，这里的前后主要是约束副本机器上的写入顺序。即，一个会话一旦某台机器上完成了写操作，无论在哪台机器上，后续会话的写操作都要排在它后面。

下图是违背先读后写的场景，用户2的更新比用户1的先到达了副本机器，副本机器先执行用户2的更新违背了先读后写：

![非写读后写]({{ site.article }}/yizhixing-4.webp)

#### 多会话：线性一致性

Linearizability，线性一致性和事务中的可串行化Serializability类似，都是试图把并发操作转换成无并发的顺序操作。即要求所有在时间上有重叠的并发操作，都能够被转换成无并发的顺序操作。即并发操作的结果，和按特定顺序操作的结果等价，如何实现这种等价在设计的时候考虑。 

## 分布式共识算法 

TODO...




## 参考

1. [李佶澳的博客][1]
2. [Developing with kdb+ and the q language][2]
3. [Mysql: ACID][3]
4. [Two-Phase Commit (2PC)][3]

[1]: https://www.lijiaocn.com "李佶澳的博客"
[2]: https://code.kx.com/q/  "Developing with kdb+ and the q language"
[3]: https://dev.mysql.com/doc/refman/8.0/en/glossary.html#glos_acid "ACID"
[4]: https://servicecomb.apache.org/docs/distributed_saga_3/ "Two-Phase Commit (2PC)"

