---
layout: default
title: "使用Nginx作反向代理，启用keepalive时，遇到502错误的调查过程 "
author: 李佶澳
createdate: "2019-06-13 13:30:00 +0800"
changedate: "2019-06-17 20:08:19 +0800"
categories: 问题
tags:  nginx
cover:
keywords: nginx,502,keepalive,keepalive_request,maxKeepAliveRequests
description: 提出的两个假设未能稳定复现，方案可能有效，意外发现是当 Nginx 上的端口耗尽时，会引发 502
---

## 本篇目录

* auto-gen TOC:
{:toc}

## 现象

Nginx 的访问日志间歇性出现 502 响应码，查看 nginx 的 error.log，发现是 upstream 返回了 reset：

```sh
2019/06/13 04:57:54 [error] 3429#3429: *21983075 upstream prematurely closed connection while reading 
response header from upstream, client: 10.19.167.120, server: XXXX.com, request: "POST XXXX HTTP/1.0",
upstream: "http://11.0.29.4:8080/XXXXXX", host: "XXXX.com"

2019/06/13 04:58:34 [error] 3063#3063: *21989359 recv() failed (104: Connection reset by peer) while 
reading response header from upstream, client: 10.19.138.139, server: XXXX.com, request: 
"POST /api/v1/XXXX HTTP/1.1", upstream: "http://11.0.145.9:8080/api/v1/XXXX", host: "XXXX.com"
```

## 调查

在 Nginx 上抓包，发现最后触发 reset 的请求的发送时间比上次晚了 1 分钟以上，nginx 向 upstream 发送请求后，upstream 直接返回 502：

![nginx 104 reset]({{ site.imglocal }}/article/nginx-104-reset-1.png)

检查其它正常连接，发现服务端在连接 idle 时间为 1分20 秒时候，主动断开连接：

![nginx 104 reset]({{ site.imglocal }}/article/nginx-104-reset-2.png)

比较奇怪的是 nginx 设置的 keepalive_timeout 为 60 秒，为什么不是 nginx 主动断开连接？而是在 60s 之后继续发送请求导致收到 reset ？

## 验证第一个假设：服务端 idle 超时先触发

猜测是 upstream 对应的服务端也设置了超时时间，并且比 nginx 先超时，因此出现了服务端率先断开连接情况。如果服务端断开连接时，nginx 正好要向 upstream 发送请求，就可能出现 reset 的情况。

部署一个服务端容器，服务端的 idle 超时设置为 10 秒，小于 nginx 中配置的 keepalive_timeout （60秒）。用 [httpperf](https://www.lijiaocn.com/%E6%96%B9%E6%B3%95/2018/11/02/webserver-benchmark-method.html#sessions%E6%A8%A1%E6%8B%9F%E7%94%A8%E6%88%B7%E4%BC%9A%E8%AF%9D) 模拟 100 个会话，每个会话以每间隔 9 秒发送 100 个请求的方式累计发送 300 个请求，会话创建速率是每秒 10 个。

```sh
httperf --server webshell.example.test --port 80 --wsess 100,300,9 --burst-len 100 --rate 10
./httperf --server webshell.example.test --port 80 --wsess 1,256,9 --burst-len 128 --rate 1
```

httperf 太老旧了，没有复现出来，假定场景本身也很难复现，必须恰好在连接因为超时关闭时发送请求，比较难复现。找到三篇文章都在讨论这个问题：

1. [104: Connection reset by peer while reading response header from upstream](https://discuss.konghq.com/t/104-connection-reset-by-peer-while-reading-response-header-from-upstream/249)
2. [HTTP 502 response generated by a proxy after it tries to send data upstream to a partially closed connection (reset packet)](https://serverfault.com/questions/845171/http-502-response-generated-by-a-proxy-after-it-tries-to-send-data-upstream-to-a/845176)
3. [Analyze ‘Connection reset’ error in Nginx upstream with keep-alive enabled](https://theantway.com/2017/11/analyze-connection-reset-error-in-nginx-upstream-with-keep-alive-enabled/)


## 发现新情况

有用户反应能够稳定复现 502，且是在压测期间发现的，这就奇怪了。idle 超时导致的 502  应该很难出现，稳定复现的 502 显然是有其它原因的。

了解到用户的服务使用 tomcat 服务，查阅 tomcat 的配置，发现了下面的[参数](https://tomcat.apache.org/tomcat-8.5-doc/config/http.html)：

![tomcat config maxKeepAliveRequests]({{ site.imglocal }}/article/nginx-104-reset-3.png)

tomcat 也有 keep-alive 配置，默认每个连接中最多 100 个请求，而 nginx 中配置的 keepalive_requests 超过了 100，这会不会是问题根源？需要测试验证一下。

同时发现 tomcat 默认的 idle 超时时间是 60s，和前面提出的超时假设能够相呼应，tomcat 7 和 tomcat 8 的使用的是相同的默认配置。

![tomcat config maxKeepAliveRequests]({{ site.imglocal }}/article/nginx-104-reset-4.png)

![tomcat config maxKeepAliveRequests]({{ site.imglocal }}/article/nginx-104-reset-5.png)

## 验证第二个假设：服务端率先断开连接会导致502

部署一个 tomcat 服务，idle 超时时间为 60s，maxKeepAliveRequests 为 100，nginx 的 idle 超时为 60s，keepalive_requests 为 2000，用下面的命令压测：

```sh
./wrk -t 4  -c 10000 -d 90s http://ka-test-tomcat.example.test/ping
```

10000 并发压测 90s 时，出现 502 响应：

![1000并发压测出现502响应]({{ site.imglocal }}/article/nginx-104-reset-6.png)

然而从报文发现：服务端直接 reset 的连接数是 24 个，数量远远低于 502  响应的数量。

怀疑还有其它原因，检查 Nginx 发现大量下面的日志：

```sh
2019/06/17 09:35:46 [crit] 28805#28805: *9114394 connect() to 10.12.4.197:8080 failed (99: Cannot assign requested address) while connecting to upstream, client: 10.10.173.203, server: ka-test-tomcat.example.test, request: "GET /ping HTTP/1.1", upstream: "http://10.12.4.197:8080/ping", host: "ka-test-tomcat.example.test"
2019/06/17 09:35:46 [crit] 28806#28806: *9114649 connect() to 10.12.4.197:8080 failed (99: Cannot assign requested address) while connecting to upstream, client: 10.10.173.203, server: ka-test-tomcat.example.test, request: "GET /ping HTTP/1.1", upstream: "http://10.12.4.197:8080/ping", host: "ka-test-tomcat.example.test"
2019/06/17 09:35:46 [crit] 28804#28804: *9114172 connect() to 10.12.4.197:8080 failed (99: Cannot assign requested address) while connecting to upstream, client: 10.10.173.203, server: ka-test-tomcat.example.test, request: "GET /ping HTTP/1.1", upstream: "http://10.12.4.197:8080/ping", host: "ka-test-tomcat.example.test"
```

统计了一下，crit 日志数基本和 502 响应的数量的量级别相同，`大部分 502 是 Nginx 上的端口不足导致的`， **又发现了一种导致 502 原因：Nginx 的端口耗尽** 。

尝试降低并发数量，排除端口耗尽的情况：

```sh
./wrk -t 4  -c 500 -d 300s http://ka-test-tomcat.example.test/ping
```

结果比较悲催，无论如何也没有 502，检查报文发现有少量服务端回应的 RST 报文，是在发起了 FIN 连接后再次回应的。
但是在多次压测过程中，观察过到两次`104: Connection reset by peer`，不知道是在什么情况下产生的......

## 初步结论

提出的两个假设，都未能稳定复现，只能是根据日志推测和原理进行推测。意外发现是当 Nginx 上的端口耗尽时，会引发 502。
只能提出两个可能有效的方案：

1. Nginx 的 keep-alive 的 idle 超时要小于 upstream 的 idle 超时；
2. Nginx 的 keepalive_request 要大于 upstream 的相关设置。

## 参考

1. [李佶澳的博客笔记][1]

[1]: https://www.lijiaocn.com "李佶澳的博客笔记"
