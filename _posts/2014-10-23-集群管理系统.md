---
layout: default
title: 怎样选择集群管理系统?
author: lijiaocn
createdate: 2014/10/23 14:01:25
changedate: 2017/10/28 12:36:03
categories: 方法
tags: kubernetes
keywords: 集群管理系统,kubernetes
description: 汇总各种公开/半公开的集群管理系统。

---

* auto-gen TOC:
{:toc}

## 汇总

汇总各种公开/半公开的集群管理系统。

### Borg

Google的第一代/第二代集群（资源）管理系统被称为Borg，Borg设计细节因零零星星出现在各种文章中而知名，但一直未公开（比如发一篇paper）。然而，我们可从腾讯公布的Torca（Torca是google华人老员工朱会灿加入搜搜后，仿照google borg开发的资源管理系统, 链接是：“Torca：Typhoon上的分布式集群调度系统”）设计文档中可猜测一二。

[说法来自这里](http://dongxicheng.org/mapreduce-nextgen/google-omega/)

### Torca架构

Torca是google的朱会灿加入搜搜后仿照google borg开发。

[Torca: Typhonn上的分布式集群调度系统](http://djt.qq.com/bbs/thread-29998-1-1.html)

Cluster由Manager Server和Execute Server组成：

	1. Cluster中的Central Manager负责调度
	2. Cluster中的Execute Server承担任务

Central Manager负责调度:

	1. Central Manager->Master Daemon: 管理进程
	2. Central Manager->Scheduler: 裁断任务的调度策略
	3. Central Manager->Collector: 收集Execute Server的机器和任务状态
	4. Central Manager->Http Server: 提供浏览器查询

Execute Server承担任务:

	1. Execute Server->Master Daemon: 管理进程
	2. Execute Server->Start Deamon: 
		1. 接收Central Manager下发的任务
		2. 创建执行任务的进程(注意: 任务进程是其子进程)
		3. 根据Central Manager指令管理任务进程
		4. 将任务进程状态上报Central Manager
		5. 定期上报物理机状态
	3. Execute Server->Http Server: 提供浏览器查询

Torca的容灾策略:

	1. Central Master使用主备机制
	2. Execute Server被监控
	3. ZooKeeper提供强一致性

使用方法:

	1. 用户通过Submitter(Torca客户端)从Central Manager获取任务状态、提交任务、管理任务等。
	2. 任务的属性和资源需求通过任务描述文件说明。
	3. 任务的依赖文件(程序体)存放在共享文件系统(XFS)中, Execute Server主动拉取。
	4. 通过名字服务(Torca naming service)获得任务的服务地址(IP:Port)

任务的容灾策略：

	1. 共享文件服务(XFS)中断导致任务的依赖文件无法获取, 正在转向Execute Server间的p2p减少XFS的依赖.
	   (评: 使用p2p没有必要, 存储的问题就应当有存储来解决) 
	2. Torca通过retry/migrate保证任务的执行。
	3. Torca将任务的异常状态实时通知到指定人员(email、短信等)。

Torca的技术问题:

	1. 任务的隔离:  在Execute Server上使用LXC技术进行隔离
	2. 资源的分配算法: [Unkown: 详情未知]
	3. 资源碎片:       [Unkown: 详情未知]
	4. 任务资源的出让和回收: [Unkown: 详情未知]
	   (任务空闲时出让计算资源, 负载上升时回收计算资源)
	5. 任务峰值时所需资源的满足:  树状container  [Unkown: 详情未知]
	6. Torca的冷升级: 任务自动迁移
	7. Torca的热升级: 记录进程的checkpoint, 升级后从checkpoint恢复

### Torca点评

Torca是集中式的集群资源管理系统, 资源的控制粒度细致到CPU、内存(详细粒度取决于LXC), 从而使整个Cluster成为一个大的资源池。

整个Cluster资源池是由一个个包含了CPU、内存等资源的资源颗粒组成(也就是物理机), 因为资源颗粒内部资源不可互通，所以会存在资源碎片的问题

资源碎片类似于磁盘碎片。

### Apache Mesos

[Apache Mesos](http://mesos.apache.org/)

Mesos最初是UC Berkeley的研究项目([mesos-paper](http://mesos.berkeley.edu/mesos_tech_report.pdf))

Mesos要解决的问题: 

	各种分布式框架部署在独立的集群中, 集群的计算资源不能有效利用。

Mesos的解决方案:

	将所有的分布式框架部署在同一个集群中, 动态调整各个框架对集群资源的占用, 实现集群的复用。

Mesos面临的问题:

	分布式框架种类多, 原理与类型存在差异, 而且未来还会有新的框架不断出现。

Mesos的处理方式:

	双层调度设计(two-level Scheduler)
	Mesos负责将可用的计算资源交付给Framework, 由Framework自行调度它所接收的资源。
	Framework使用完毕后将资源归还给Mesos, Mesos将其分配给其它的Framework, 从而提高资源的利用率。

Mesos的架构:

	1 Master负责计算资源在Framework间的分配
	2 Slave承担实际的计算任务

Mesos的任务隔离:

	在Slave上使用容器技术进行隔离。

Mesos的容灾:

	1. Master使用主备方式容灾
	2. zookeeper

Job的容灾:

	Job的容灾由Framework解决

Mesos的使用:

	1. 注册Framework: 将实现了调度策略的Scheduler和承担task的Executer提交到Mesos.
	2. 提交Job:  将Job提交到注册在Mesos中的Framework

在Mesos的论文中给出的实验数据显示, 相比将集群分割成多个小集群的方式, 采用Mesos的集群的CPU利用率提高10%,内存利用率提高18%。

Framework的工作的效率根据任务类型的不同各有升降, 最差的情况降低了20%, 最好情况提高了100%。

可以再Mesos上运行的Framework:

[Frameworks on Mesos](http://mesos.apache.org/documentation/latest/mesos-frameworks/)

### Apache Mesos点评

Mesos将资源分配给framwork后, framework自行管理, framework内部的状态对Mesos来讲是不可见的。Framework需要能够自觉的释放多余资源。

假设有这样一个framework, 这个framework中运行的100个web server, 每个web server都会占据一个计算单元。当web server的工作不饱和, 只需要10个web server就可以的时候, framework需要自觉的将其中的90个web server的资源归还。

>web server与hadoop中的task是两类不同的task, 前者需要长期稳定的运行, 后者需要快速执行返回。Mesos的设计更有利于后者。

另一种解决思路是将framework部署在虚拟机中, 资源利用率的问题交给虚拟机管理平台解决。这种方式不需要对framework作改动。但是同样存在自觉释放资源的问题, 因为虚拟机管理平台也不知道应当何时对虚拟机进行增减。

>如果采用部署在虚拟机中方式, 当虚拟机的负载较低时是否有将其释放的必要?如果不释放，会浪费哪些资源?

另外在Omega的论文中指出, Mesos对资源采用了悲观锁设计, 并发性不好。

### YARN

YARN是Hadoop项目中的一个通用的调度系统, 是下一代Map-Reduce, 也称为MRv2。

[YARN](http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YARN.html)

### Omega

Omega是google内部使用的集群管理调度系统, 在2013的一片论文中被披露。

[Omega-paper](http://eurosys2013.tudos.org/wp-content/uploads/2013/paper/Schwarzkopf.pdf)

Omega依然是two-level的设计, 资源的具体调度由各自的scheduler负责。与Mesos的不同是获取资源的方式:

	Mesos: Master向Framework发送Resource Offer
	Omega: scheduler可以看到全局状态, 直接抢占资源。

Omega中维护了一个集群的状态(cell state), 每个scheduler直接抢占资源，使用MVCC协议进行并发控制。

### Omega点评

从Mesos到Omega, scheduler主动性进一步增加, 不仅仅自主管理已经获取的资源, 而且可以干涉整个集群的资源, 例如抢占优先级低scheduler的资源。

需要对scheduler严加控制，防止因为某个scheduler的故障，造成整个集群的不稳定。

### Kubernetes

[Kubernetes](https://github.com/GoogleCloudPlatform/kubernetes)

与上面介绍的集群调度系统不同的是Kubernetes侧重的是docker的调度管理。

Kubernetes的组成部分：

	kube::etcd::start --> etcd -name test --data-dir ?? -addr ??
	apiserver -v=? --address=? --port=? --etcd_servers=? --portal_net=? --cors_allowed_origins
	controller-manager -v=? --machines=? --master=?
	kubelet -v=? --etcd_servers=? --hostname_override=? --address=? --port=?
	proxy -v=? --master=?
	scheduler -v=? --master=?
